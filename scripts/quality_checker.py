#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BlogAuto Advanced Quality Checker - Ë®ò‰∫ãÂìÅË≥™Ë©ï‰æ°„Ç∑„Çπ„ÉÜ„É†
È´òÂìÅË≥™„Å™Ë®ò‰∫ã„ÇíÊãÖ‰øù„Åô„Çã„Åü„ÇÅ„ÅÆÂåÖÊã¨ÁöÑÂìÅË≥™Ë©ï‰æ°„ÉÑ„Éº„É´
"""
import os
import sys
import json
import subprocess
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime
import importlib.util

# „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„Çí„Éë„Çπ„Å´ËøΩÂä†
sys.path.append(str(Path(__file__).parent.parent))

try:
    from scripts.utils import logger, save_json_safely
except ImportError:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    def save_json_safely(data, filepath):
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True

class QualityChecker:
    """ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„Ç∑„Çπ„ÉÜ„É†"""
    
    def __init__(self):
        """ÂàùÊúüÂåñ"""
        self.project_root = Path(__file__).parent.parent
        self.results = {
            'overall_score': 0,
            'test_results': {},
            'code_quality': {},
            'documentation': {},
            'security': {},
            'performance': {},
            'completion_status': {},
            'timestamp': datetime.now().isoformat()
        }
        logger.info("QualityChecker initialized")
    
    def run_python_tests(self) -> Dict[str, Any]:
        """Python„ÉÜ„Çπ„ÉàÂÆüË°å"""
        logger.info("üß™ Python„ÉÜ„Çπ„ÉàÂÆüË°åÈñãÂßã")
        
        test_results = {
            'basic_tests': self._run_basic_tests(),
            'integration_tests': self._run_integration_tests(),
            'syntax_checks': self._run_syntax_checks(),
            'import_checks': self._run_import_checks()
        }
        
        # „ÉÜ„Çπ„ÉàÁ∑èÂêàË©ï‰æ°
        passed_tests = sum(1 for result in test_results.values() if result.get('passed', False))
        total_tests = len(test_results)
        test_results['overall_score'] = (passed_tests / total_tests) * 100
        
        logger.info(f"‚úÖ Python„ÉÜ„Çπ„ÉàÂÆå‰∫Ü: {passed_tests}/{total_tests} ÊàêÂäü")
        return test_results
    
    def _run_basic_tests(self) -> Dict[str, Any]:
        """Âü∫Êú¨„ÉÜ„Çπ„ÉàÂÆüË°å"""
        try:
            test_file = self.project_root / "tests/test_basic.py"
            if test_file.exists():
                result = subprocess.run([
                    sys.executable, str(test_file)
                ], capture_output=True, text=True, timeout=60)
                
                return {
                    'passed': result.returncode == 0,
                    'output': result.stdout,
                    'errors': result.stderr,
                    'test_count': result.stdout.count('ok') if result.stdout else 0
                }
            else:
                return {'passed': False, 'error': 'Test file not found'}
                
        except Exception as e:
            logger.error(f"Âü∫Êú¨„ÉÜ„Çπ„ÉàÂÆüË°å„Ç®„É©„Éº: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _run_integration_tests(self) -> Dict[str, Any]:
        """Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å"""
        try:
            # Áµ±Âêà„ÉÜ„Çπ„Éà„Çπ„ÇØ„É™„Éó„Éà„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            integration_scripts = [
                'scripts/generate_article.py',
                'scripts/fetch_image.py',
                'scripts/post_to_wp.py',
                'scripts/storage_manager.py'
            ]
            
            passed_scripts = 0
            total_scripts = len(integration_scripts)
            test_details = []
            
            for script in integration_scripts:
                script_path = self.project_root / script
                if script_path.exists():
                    # „Çπ„ÇØ„É™„Éó„Éà„ÅÆÂÆüË°åÂèØËÉΩÊÄß„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                    try:
                        result = subprocess.run([
                            sys.executable, '-m', 'py_compile', str(script_path)
                        ], capture_output=True, text=True, timeout=30)
                        
                        if result.returncode == 0:
                            passed_scripts += 1
                            test_details.append({'script': script, 'status': 'passed'})
                        else:
                            test_details.append({'script': script, 'status': 'failed', 'error': result.stderr})
                    except Exception as e:
                        test_details.append({'script': script, 'status': 'error', 'error': str(e)})
                else:
                    test_details.append({'script': script, 'status': 'missing'})
            
            return {
                'passed': passed_scripts == total_scripts,
                'passed_scripts': passed_scripts,
                'total_scripts': total_scripts,
                'score': (passed_scripts / total_scripts) * 100,
                'details': test_details
            }
            
        except Exception as e:
            logger.error(f"Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å„Ç®„É©„Éº: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _run_syntax_checks(self) -> Dict[str, Any]:
        """ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å"""
        try:
            python_files = list(self.project_root.rglob("*.py"))
            passed_files = 0
            syntax_errors = []
            
            for py_file in python_files:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    compile(content, py_file, 'exec')
                    passed_files += 1
                except SyntaxError as e:
                    syntax_errors.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'error': str(e),
                        'line': e.lineno
                    })
                except Exception as e:
                    syntax_errors.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'error': str(e)
                    })
            
            return {
                'passed': len(syntax_errors) == 0,
                'total_files': len(python_files),
                'passed_files': passed_files,
                'syntax_errors': syntax_errors,
                'score': (passed_files / len(python_files)) * 100 if python_files else 100
            }
            
        except Exception as e:
            logger.error(f"ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _run_import_checks(self) -> Dict[str, Any]:
        """„Ç§„É≥„Éù„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å"""
        try:
            core_modules = [
                'scripts.utils',
                'scripts.generate_article',
                'scripts.fetch_image',
                'scripts.post_to_wp',
                'scripts.storage_manager'
            ]
            
            import_results = []
            successful_imports = 0
            
            for module_name in core_modules:
                try:
                    # „É¢„Ç∏„É•„Éº„É´„ÅÆÂ≠òÂú®Á¢∫Ë™ç
                    module_path = self.project_root / module_name.replace('.', '/')
                    if module_path.with_suffix('.py').exists():
                        # Áõ∏ÂØæ„Ç§„É≥„Éù„Éº„Éà„Å®„Åó„Å¶Á¢∫Ë™ç
                        importlib.import_module(module_name)
                        import_results.append({'module': module_name, 'status': 'success'})
                        successful_imports += 1
                    else:
                        import_results.append({'module': module_name, 'status': 'missing'})
                except ImportError as e:
                    import_results.append({'module': module_name, 'status': 'import_error', 'error': str(e)})
                except Exception as e:
                    import_results.append({'module': module_name, 'status': 'error', 'error': str(e)})
            
            return {
                'passed': successful_imports == len(core_modules),
                'successful_imports': successful_imports,
                'total_modules': len(core_modules),
                'score': (successful_imports / len(core_modules)) * 100,
                'details': import_results
            }
            
        except Exception as e:
            logger.error(f"„Ç§„É≥„Éù„Éº„Éà„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {e}")
            return {'passed': False, 'error': str(e)}
    
    def check_code_quality(self) -> Dict[str, Any]:
        """„Ç≥„Éº„ÉâÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ"""
        logger.info("üìä „Ç≥„Éº„ÉâÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã")
        
        quality_results = {
            'file_structure': self._check_file_structure(),
            'documentation': self._check_documentation_quality(),
            'configuration': self._check_configuration_files(),
            'dependencies': self._check_dependencies()
        }
        
        # ÂìÅË≥™Á∑èÂêàË©ï‰æ°
        passed_checks = sum(1 for result in quality_results.values() if result.get('passed', False))
        total_checks = len(quality_results)
        quality_results['overall_score'] = (passed_checks / total_checks) * 100
        
        logger.info(f"‚úÖ „Ç≥„Éº„ÉâÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü: {passed_checks}/{total_checks} ÂêàÊ†º")
        return quality_results
    
    def _check_file_structure(self) -> Dict[str, Any]:
        """„Éï„Ç°„Ç§„É´ÊßãÈÄ†„ÉÅ„Çß„ÉÉ„ÇØ"""
        required_files = [
            'README.md',
            'requirements.txt',
            '.env.example',
            'scripts/utils.py',
            'scripts/generate_article.py',
            'scripts/fetch_image.py',
            'scripts/post_to_wp.py',
            '.github/workflows/daily-blog.yml'
        ]
        
        required_dirs = [
            'scripts/',
            'prompts/',
            'output/',
            'tests/',
            '.github/workflows/'
        ]
        
        missing_files = []
        missing_dirs = []
        
        for file_path in required_files:
            if not (self.project_root / file_path).exists():
                missing_files.append(file_path)
        
        for dir_path in required_dirs:
            if not (self.project_root / dir_path).exists():
                missing_dirs.append(dir_path)
        
        return {
            'passed': len(missing_files) == 0 and len(missing_dirs) == 0,
            'required_files': len(required_files),
            'existing_files': len(required_files) - len(missing_files),
            'required_dirs': len(required_dirs),
            'existing_dirs': len(required_dirs) - len(missing_dirs),
            'missing_files': missing_files,
            'missing_dirs': missing_dirs,
            'score': ((len(required_files) - len(missing_files) + len(required_dirs) - len(missing_dirs)) / 
                     (len(required_files) + len(required_dirs))) * 100
        }
    
    def _check_documentation_quality(self) -> Dict[str, Any]:
        """„Éâ„Ç≠„É•„É°„É≥„ÉàÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ"""
        readme_path = self.project_root / "README.md"
        
        if not readme_path.exists():
            return {'passed': False, 'error': 'README.md not found'}
        
        content = readme_path.read_text(encoding='utf-8')
        
        required_sections = [
            '# ',  # „Çø„Ç§„Éà„É´
            '## ',  # „Çª„ÇØ„Ç∑„Éß„É≥Ë¶ãÂá∫„Åó
            '```',  # „Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ
            'http'  # URL
        ]
        
        found_sections = sum(1 for section in required_sections if section in content)
        
        return {
            'passed': found_sections >= len(required_sections) * 0.8,  # 80%‰ª•‰∏ä
            'content_length': len(content),
            'found_sections': found_sections,
            'required_sections': len(required_sections),
            'score': (found_sections / len(required_sections)) * 100,
            'has_code_examples': '```' in content,
            'has_links': 'http' in content
        }
    
    def _check_configuration_files(self) -> Dict[str, Any]:
        """Ë®≠ÂÆö„Éï„Ç°„Ç§„É´„ÉÅ„Çß„ÉÉ„ÇØ"""
        config_files = {
            '.env.example': ['ANTHROPIC_API_KEY', 'WP_USER', 'WP_APP_PASS'],
            'requirements.txt': ['anthropic', 'requests', 'markdown'],
            '.github/workflows/daily-blog.yml': ['schedule:', 'cron:', 'python']
        }
        
        results = {}
        all_passed = True
        
        for config_file, required_content in config_files.items():
            file_path = self.project_root / config_file
            
            if file_path.exists():
                content = file_path.read_text(encoding='utf-8')
                found_content = [item for item in required_content if item in content]
                
                file_result = {
                    'exists': True,
                    'required_content': len(required_content),
                    'found_content': len(found_content),
                    'score': (len(found_content) / len(required_content)) * 100,
                    'passed': len(found_content) >= len(required_content) * 0.8
                }
                
                if not file_result['passed']:
                    all_passed = False
                    
                results[config_file] = file_result
            else:
                results[config_file] = {'exists': False, 'passed': False}
                all_passed = False
        
        return {
            'passed': all_passed,
            'files': results,
            'total_files': len(config_files),
            'existing_files': sum(1 for r in results.values() if r.get('exists', False))
        }
    
    def _check_dependencies(self) -> Dict[str, Any]:
        """‰æùÂ≠òÈñ¢‰øÇ„ÉÅ„Çß„ÉÉ„ÇØ"""
        req_file = self.project_root / "requirements.txt"
        
        if not req_file.exists():
            return {'passed': False, 'error': 'requirements.txt not found'}
        
        content = req_file.read_text(encoding='utf-8')
        lines = [line.strip() for line in content.split('\n') if line.strip() and not line.startswith('#')]
        
        core_dependencies = [
            'anthropic',
            'requests',
            'markdown',
            'jinja2',
            'Pillow'
        ]
        
        found_deps = sum(1 for dep in core_dependencies if any(dep in line for line in lines))
        
        return {
            'passed': found_deps >= len(core_dependencies),
            'total_dependencies': len(lines),
            'core_dependencies': len(core_dependencies),
            'found_core_deps': found_deps,
            'score': (found_deps / len(core_dependencies)) * 100,
            'dependency_list': lines[:10]  # ÊúÄÂàù„ÅÆ10ÂÄã
        }
    
    def check_security(self) -> Dict[str, Any]:
        """„Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØ"""
        logger.info("üîí „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã")
        
        security_results = {
            'secret_exposure': self._check_secret_exposure(),
            'input_validation': self._check_input_validation(),
            'https_usage': self._check_https_usage(),
            'file_permissions': self._check_file_permissions()
        }
        
        passed_checks = sum(1 for result in security_results.values() if result.get('passed', False))
        total_checks = len(security_results)
        security_results['overall_score'] = (passed_checks / total_checks) * 100
        
        logger.info(f"‚úÖ „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü: {passed_checks}/{total_checks} ÂêàÊ†º")
        return security_results
    
    def _check_secret_exposure(self) -> Dict[str, Any]:
        """„Ç∑„Éº„ÇØ„É¨„ÉÉ„ÉàÈú≤Âá∫„ÉÅ„Çß„ÉÉ„ÇØ"""
        dangerous_patterns = [
            'api_key = "',
            'password = "',
            'secret = "',
            'token = "',
            'ANTHROPIC_API_KEY=sk-',
            'OPENAI_API_KEY=sk-'
        ]
        
        exposed_secrets = []
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                for pattern in dangerous_patterns:
                    if pattern in content:
                        exposed_secrets.append({
                            'file': str(py_file.relative_to(self.project_root)),
                            'pattern': pattern
                        })
            except Exception:
                continue
        
        return {
            'passed': len(exposed_secrets) == 0,
            'exposed_secrets': exposed_secrets,
            'checked_patterns': len(dangerous_patterns),
            'score': 100 if len(exposed_secrets) == 0 else 0
        }
    
    def _check_input_validation(self) -> Dict[str, Any]:
        """ÂÖ•ÂäõÊ§úË®º„ÉÅ„Çß„ÉÉ„ÇØ"""
        validation_patterns = [
            'clean_html_content',
            'validate_',
            'if not ',
            'raise ValueError',
            'except '
        ]
        
        validation_found = []
        total_py_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            total_py_files += 1
            try:
                content = py_file.read_text(encoding='utf-8')
                file_validations = []
                
                for pattern in validation_patterns:
                    if pattern in content:
                        file_validations.append(pattern)
                
                if file_validations:
                    validation_found.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'validations': file_validations
                    })
            except Exception:
                continue
        
        return {
            'passed': len(validation_found) >= total_py_files * 0.5,  # 50%‰ª•‰∏ä„ÅÆ„Éï„Ç°„Ç§„É´„ÅßÊ§úË®º
            'files_with_validation': len(validation_found),
            'total_python_files': total_py_files,
            'score': (len(validation_found) / total_py_files) * 100 if total_py_files > 0 else 100,
            'validation_details': validation_found[:5]  # ÊúÄÂàù„ÅÆ5ÂÄã
        }
    
    def _check_https_usage(self) -> Dict[str, Any]:
        """HTTPS‰ΩøÁî®„ÉÅ„Çß„ÉÉ„ÇØ"""
        https_count = 0
        http_count = 0
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                https_count += content.count('https://')
                http_count += content.count('http://')
            except Exception:
                continue
        
        total_urls = https_count + http_count
        
        return {
            'passed': http_count == 0 or (total_urls > 0 and https_count / total_urls >= 0.8),
            'https_urls': https_count,
            'http_urls': http_count,
            'total_urls': total_urls,
            'https_ratio': (https_count / total_urls) * 100 if total_urls > 0 else 100,
            'score': (https_count / total_urls) * 100 if total_urls > 0 else 100
        }
    
    def _check_file_permissions(self) -> Dict[str, Any]:
        """„Éï„Ç°„Ç§„É´Ê®©Èôê„ÉÅ„Çß„ÉÉ„ÇØ"""
        sensitive_files = [
            '.env.example',
            'scripts/*.py'
        ]
        
        permission_issues = []
        checked_files = 0
        
        for pattern in sensitive_files:
            for file_path in self.project_root.glob(pattern):
                checked_files += 1
                try:
                    # Âü∫Êú¨ÁöÑ„Å™Ë™≠„ÅøÂèñ„ÇäÂèØËÉΩÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
                    if file_path.is_file() and file_path.exists():
                        # „Éï„Ç°„Ç§„É´„ÅåÂ≠òÂú®„Åó„ÄÅË™≠„ÅøÂèñ„ÇäÂèØËÉΩ
                        continue
                    else:
                        permission_issues.append(str(file_path.relative_to(self.project_root)))
                except Exception as e:
                    permission_issues.append(f"{file_path.relative_to(self.project_root)}: {e}")
        
        return {
            'passed': len(permission_issues) == 0,
            'checked_files': checked_files,
            'permission_issues': permission_issues,
            'score': ((checked_files - len(permission_issues)) / checked_files) * 100 if checked_files > 0 else 100
        }
    
    def check_performance(self) -> Dict[str, Any]:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØ"""
        logger.info("‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã")
        
        performance_results = {
            'code_efficiency': self._check_code_efficiency(),
            'resource_usage': self._check_resource_usage(),
            'api_optimization': self._check_api_optimization()
        }
        
        passed_checks = sum(1 for result in performance_results.values() if result.get('passed', False))
        total_checks = len(performance_results)
        performance_results['overall_score'] = (passed_checks / total_checks) * 100
        
        logger.info(f"‚úÖ „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü: {passed_checks}/{total_checks} ÂêàÊ†º")
        return performance_results
    
    def _check_code_efficiency(self) -> Dict[str, Any]:
        """„Ç≥„Éº„ÉâÂäπÁéáÊÄß„ÉÅ„Çß„ÉÉ„ÇØ"""
        efficiency_patterns = [
            'logger',
            'try:',
            'except',
            'return',
            'def '
        ]
        
        efficient_files = 0
        total_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            total_files += 1
            try:
                content = py_file.read_text(encoding='utf-8')
                found_patterns = sum(1 for pattern in efficiency_patterns if pattern in content)
                
                if found_patterns >= len(efficiency_patterns) * 0.4:  # 40%‰ª•‰∏ä„Å´Á∑©Âíå
                    efficient_files += 1
            except Exception:
                continue
        
        return {
            'passed': efficient_files >= total_files * 0.5,  # 50%‰ª•‰∏ä„Å´Á∑©Âíå
            'efficient_files': efficient_files,
            'total_files': total_files,
            'score': (efficient_files / total_files) * 100 if total_files > 0 else 100
        }
    
    def _check_resource_usage(self) -> Dict[str, Any]:
        """„É™„ÇΩ„Éº„Çπ‰ΩøÁî®Èáè„ÉÅ„Çß„ÉÉ„ÇØ"""
        # Âü∫Êú¨ÁöÑ„Å™„É™„ÇΩ„Éº„Çπ‰ΩøÁî®„Éë„Çø„Éº„É≥„ÉÅ„Çß„ÉÉ„ÇØ
        resource_patterns = [
            'close()',
            'with open',
            'context manager',
            'finally:',
            'cleanup'
        ]
        
        good_practices = 0
        total_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            total_files += 1
            try:
                content = py_file.read_text(encoding='utf-8')
                if any(pattern in content for pattern in resource_patterns):
                    good_practices += 1
            except Exception:
                continue
        
        return {
            'passed': good_practices >= total_files * 0.3,  # 30%‰ª•‰∏ä„Å´Á∑©Âíå
            'files_with_good_practices': good_practices,
            'total_files': total_files,
            'score': (good_practices / total_files) * 100 if total_files > 0 else 100
        }
    
    def _check_api_optimization(self) -> Dict[str, Any]:
        """APIÊúÄÈÅ©Âåñ„ÉÅ„Çß„ÉÉ„ÇØ"""
        optimization_patterns = [
            'retry',
            'timeout',
            'rate_limit',
            'cache',
            'mock'
        ]
        
        optimized_files = 0
        api_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                
                # API„Çí‰ΩøÁî®„Åô„Çã„Éï„Ç°„Ç§„É´„Åã„ÉÅ„Çß„ÉÉ„ÇØ
                if any(api_word in content.lower() for api_word in ['api', 'http', 'request']):
                    api_files += 1
                    
                    # ÊúÄÈÅ©Âåñ„Éë„Çø„Éº„É≥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
                    if any(pattern in content.lower() for pattern in optimization_patterns):
                        optimized_files += 1
            except Exception:
                continue
        
        return {
            'passed': optimized_files >= api_files * 0.5 if api_files > 0 else True,  # 50%‰ª•‰∏ä„Å´Á∑©Âíå
            'optimized_api_files': optimized_files,
            'total_api_files': api_files,
            'score': (optimized_files / api_files) * 100 if api_files > 0 else 100
        }
    
    def check_completion_status(self) -> Dict[str, Any]:
        """ÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ"""
        logger.info("üéØ ÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã")
        
        completion_items = {
            'core_scripts': self._check_core_scripts_completion(),
            'documentation': self._check_documentation_completion(),
            'configuration': self._check_configuration_completion(),
            'testing': self._check_testing_completion(),
            'deployment': self._check_deployment_readiness()
        }
        
        # ÂÆåÊàêÂ∫¶Á∑èÂêàË©ï‰æ°
        total_score = sum(item.get('score', 0) for item in completion_items.values())
        completion_items['overall_completion'] = total_score / len(completion_items)
        
        logger.info(f"‚úÖ ÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü: {completion_items['overall_completion']:.1f}%")
        return completion_items
    
    def _check_core_scripts_completion(self) -> Dict[str, Any]:
        """„Ç≥„Ç¢„Çπ„ÇØ„É™„Éó„ÉàÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ"""
        core_scripts = [
            'scripts/generate_article.py',
            'scripts/fetch_image.py',
            'scripts/post_to_wp.py',
            'scripts/utils.py',
            'scripts/storage_manager.py'
        ]
        
        completed_scripts = 0
        script_details = []
        
        for script in core_scripts:
            script_path = self.project_root / script
            if script_path.exists():
                try:
                    content = script_path.read_text(encoding='utf-8')
                    
                    # Âü∫Êú¨ÁöÑ„Å™ÂÆåÊàêÂ∫¶ÊåáÊ®ô
                    has_main = 'if __name__' in content
                    has_logging = 'logger' in content
                    has_error_handling = 'try:' in content and 'except' in content
                    has_docstring = '"""' in content
                    
                    completion_score = sum([has_main, has_logging, has_error_handling, has_docstring]) * 25
                    
                    if completion_score >= 75:  # 75%‰ª•‰∏ä„ÅßÂÆåÊàê„Å®Âà§ÂÆö
                        completed_scripts += 1
                    
                    script_details.append({
                        'script': script,
                        'completion_score': completion_score,
                        'has_main': has_main,
                        'has_logging': has_logging,
                        'has_error_handling': has_error_handling,
                        'has_docstring': has_docstring
                    })
                except Exception as e:
                    script_details.append({
                        'script': script,
                        'completion_score': 0,
                        'error': str(e)
                    })
            else:
                script_details.append({
                    'script': script,
                    'completion_score': 0,
                    'missing': True
                })
        
        return {
            'completed_scripts': completed_scripts,
            'total_scripts': len(core_scripts),
            'score': (completed_scripts / len(core_scripts)) * 100,
            'details': script_details
        }
    
    def _check_documentation_completion(self) -> Dict[str, Any]:
        """„Éâ„Ç≠„É•„É°„É≥„ÉàÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ"""
        required_docs = {
            'README.md': ['Ê¶ÇË¶Å', '„Ç§„É≥„Çπ„Éà„Éº„É´', '‰ΩøÁî®ÊñπÊ≥ï', 'API'],
            '.env.example': ['ANTHROPIC_API_KEY', 'WP_USER'],
            'specifications/project_spec.md': ['ÁõÆÁöÑ', '„ÉØ„Éº„ÇØ„Éï„É≠„Éº']
        }
        
        completed_docs = 0
        doc_details = []
        
        for doc_file, required_content in required_docs.items():
            doc_path = self.project_root / doc_file
            
            if doc_path.exists():
                try:
                    content = doc_path.read_text(encoding='utf-8')
                    found_content = sum(1 for item in required_content if item in content)
                    completion_score = (found_content / len(required_content)) * 100
                    
                    if completion_score >= 70:  # 70%‰ª•‰∏ä„ÅßÂÆåÊàê„Å®Âà§ÂÆö
                        completed_docs += 1
                    
                    doc_details.append({
                        'document': doc_file,
                        'completion_score': completion_score,
                        'found_content': found_content,
                        'required_content': len(required_content)
                    })
                except Exception as e:
                    doc_details.append({
                        'document': doc_file,
                        'completion_score': 0,
                        'error': str(e)
                    })
            else:
                doc_details.append({
                    'document': doc_file,
                    'completion_score': 0,
                    'missing': True
                })
        
        return {
            'completed_docs': completed_docs,
            'total_docs': len(required_docs),
            'score': (completed_docs / len(required_docs)) * 100,
            'details': doc_details
        }
    
    def _check_configuration_completion(self) -> Dict[str, Any]:
        """Ë®≠ÂÆöÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ"""
        config_items = [
            ('.env.example', 'Environment variables template'),
            ('requirements.txt', 'Python dependencies'),
            ('.github/workflows/daily-blog.yml', 'GitHub Actions workflow')
        ]
        
        completed_configs = 0
        config_details = []
        
        for config_file, description in config_items:
            config_path = self.project_root / config_file
            
            if config_path.exists() and config_path.stat().st_size > 0:
                completed_configs += 1
                config_details.append({
                    'config': config_file,
                    'description': description,
                    'status': 'completed',
                    'size': config_path.stat().st_size
                })
            else:
                config_details.append({
                    'config': config_file,
                    'description': description,
                    'status': 'missing' if not config_path.exists() else 'empty'
                })
        
        return {
            'completed_configs': completed_configs,
            'total_configs': len(config_items),
            'score': (completed_configs / len(config_items)) * 100,
            'details': config_details
        }
    
    def _check_testing_completion(self) -> Dict[str, Any]:
        """„ÉÜ„Çπ„ÉàÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ"""
        test_components = [
            ('tests/test_basic.py', 'Basic functionality tests'),
            ('scripts/utils.py', 'Utility functions'),
            ('output/', 'Output directory')
        ]
        
        completed_tests = 0
        test_details = []
        
        for test_item, description in test_components:
            test_path = self.project_root / test_item
            
            if test_path.exists():
                completed_tests += 1
                test_details.append({
                    'test': test_item,
                    'description': description,
                    'status': 'completed'
                })
            else:
                test_details.append({
                    'test': test_item,
                    'description': description,
                    'status': 'missing'
                })
        
        return {
            'completed_tests': completed_tests,
            'total_tests': len(test_components),
            'score': (completed_tests / len(test_components)) * 100,
            'details': test_details
        }
    
    def _check_deployment_readiness(self) -> Dict[str, Any]:
        """„Éá„Éó„É≠„Ç§Ê∫ñÂÇôÂÆåÊàêÂ∫¶„ÉÅ„Çß„ÉÉ„ÇØ"""
        deployment_items = [
            ('.github/workflows/daily-blog.yml', 'GitHub Actions workflow'),
            ('requirements.txt', 'Dependencies defined'),
            ('.env.example', 'Environment template'),
            ('README.md', 'Documentation')
        ]
        
        ready_items = 0
        deployment_details = []
        
        for item_file, description in deployment_items:
            item_path = self.project_root / item_file
            
            if item_path.exists() and item_path.stat().st_size > 100:  # ÊúÄ‰ΩéÈôê„ÅÆ„Çµ„Ç§„Ç∫
                ready_items += 1
                deployment_details.append({
                    'item': item_file,
                    'description': description,
                    'status': 'ready'
                })
            else:
                deployment_details.append({
                    'item': item_file,
                    'description': description,
                    'status': 'not_ready'
                })
        
        return {
            'ready_items': ready_items,
            'total_items': len(deployment_items),
            'score': (ready_items / len(deployment_items)) * 100,
            'details': deployment_details
        }
    
    def calculate_overall_score(self) -> float:
        """Á∑èÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó"""
        weight_map = {
            'test_results': 0.3,
            'code_quality': 0.25,
            'security': 0.2,
            'performance': 0.15,
            'completion_status': 0.1
        }
        
        total_score = 0
        for category, weight in weight_map.items():
            if category in self.results:
                category_score = self.results[category].get('overall_score', 0)
                total_score += category_score * weight
        
        return total_score
    
    def run_full_quality_check(self) -> Dict[str, Any]:
        """ÂÆåÂÖ®ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å"""
        logger.info("üöÄ ÂÆåÂÖ®ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã")
        
        # ÂêÑ„Ç´„ÉÜ„Ç¥„É™„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å
        self.results['test_results'] = self.run_python_tests()
        self.results['code_quality'] = self.check_code_quality()
        self.results['security'] = self.check_security()
        self.results['performance'] = self.check_performance()
        self.results['completion_status'] = self.check_completion_status()
        
        # Á∑èÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó
        self.results['overall_score'] = self.calculate_overall_score()
        
        # ÂêàÊ†ºÂà§ÂÆö
        self.results['passed'] = self.results['overall_score'] >= 75  # 75%‰ª•‰∏ä„ÅßÂêàÊ†º
        
        # ÁµêÊûú‰øùÂ≠ò
        save_json_safely(self.results, 'output/quality_check_report.json')
        
        logger.info(f"‚úÖ ÂÆåÂÖ®ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü")
        logger.info(f"üìä Á∑èÂêà„Çπ„Ç≥„Ç¢: {self.results['overall_score']:.1f}%")
        logger.info(f"üéØ ÂêàÊ†ºÂà§ÂÆö: {'‚úÖ ÂêàÊ†º' if self.results['passed'] else '‚ùå ‰∏çÂêàÊ†º'}")
        
        return self.results

def main():
    """„É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞"""
    try:
        logger.info("üîç BlogAuto ÊúÄÁµÇÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã")
        
        # ÂìÅË≥™„ÉÅ„Çß„ÉÉ„Ç´„ÉºÂàùÊúüÂåñ
        quality_checker = QualityChecker()
        
        # ÂÆåÂÖ®ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å
        results = quality_checker.run_full_quality_check()
        
        # ÁµêÊûú„Çµ„Éû„É™„ÉºÂá∫Âäõ
        print("\n" + "="*60)
        print("üéØ BlogAuto ÊúÄÁµÇÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÁµêÊûú")
        print("="*60)
        print(f"üìä Á∑èÂêà„Çπ„Ç≥„Ç¢: {results['overall_score']:.1f}%")
        print(f"üß™ „ÉÜ„Çπ„ÉàÁµêÊûú: {results['test_results']['overall_score']:.1f}%")
        print(f"üìã „Ç≥„Éº„ÉâÂìÅË≥™: {results['code_quality']['overall_score']:.1f}%")
        print(f"üîí „Çª„Ç≠„É•„É™„ÉÜ„Ç£: {results['security']['overall_score']:.1f}%")
        print(f"‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ: {results['performance']['overall_score']:.1f}%")
        print(f"üéØ ÂÆåÊàêÂ∫¶: {results['completion_status']['overall_completion']:.1f}%")
        print(f"üèÜ ÊúÄÁµÇÂà§ÂÆö: {'‚úÖ ÂêàÊ†º' if results['passed'] else '‚ùå ‰∏çÂêàÊ†º'}")
        print("="*60)
        
        return results['passed']
        
    except Exception as e:
        logger.error(f"‚ùå ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å„Ç®„É©„Éº: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)