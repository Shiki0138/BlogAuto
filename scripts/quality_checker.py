#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BlogAuto Advanced Quality Checker - è¨˜äº‹å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
é«˜å“è³ªãªè¨˜äº‹ã‚’æ‹…ä¿ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„å“è³ªè©•ä¾¡ãƒ„ãƒ¼ãƒ«
"""
import os
import sys
import json
import subprocess
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime
import importlib.util

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

try:
    from scripts.utils import logger, save_json_safely
except ImportError:
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    def save_json_safely(data, filepath):
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True

class QualityChecker:
    """å“è³ªãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.project_root = Path(__file__).parent.parent
        self.results = {
            'overall_score': 0,
            'test_results': {},
            'code_quality': {},
            'documentation': {},
            'security': {},
            'performance': {},
            'completion_status': {},
            'timestamp': datetime.now().isoformat()
        }
        logger.info("QualityChecker initialized")
    
    def run_python_tests(self) -> Dict[str, Any]:
        """Pythonãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸ§ª Pythonãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹")
        
        test_results = {
            'basic_tests': self._run_basic_tests(),
            'integration_tests': self._run_integration_tests(),
            'syntax_checks': self._run_syntax_checks(),
            'import_checks': self._run_import_checks()
        }
        
        # ãƒ†ã‚¹ãƒˆç·åˆè©•ä¾¡
        passed_tests = sum(1 for result in test_results.values() if result.get('passed', False))
        total_tests = len(test_results)
        test_results['overall_score'] = (passed_tests / total_tests) * 100
        
        logger.info(f"âœ… Pythonãƒ†ã‚¹ãƒˆå®Œäº†: {passed_tests}/{total_tests} æˆåŠŸ")
        return test_results
    
    def _run_basic_tests(self) -> Dict[str, Any]:
        """åŸºæœ¬ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            test_file = self.project_root / "tests/test_basic.py"
            if test_file.exists():
                result = subprocess.run([
                    sys.executable, str(test_file)
                ], capture_output=True, text=True, timeout=60)
                
                return {
                    'passed': result.returncode == 0,
                    'output': result.stdout,
                    'errors': result.stderr,
                    'test_count': result.stdout.count('ok') if result.stdout else 0
                }
            else:
                return {'passed': False, 'error': 'Test file not found'}
                
        except Exception as e:
            logger.error(f"åŸºæœ¬ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _run_integration_tests(self) -> Dict[str, Any]:
        """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            # çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
            integration_scripts = [
                'scripts/generate_article.py',
                'scripts/fetch_image.py',
                'scripts/post_to_wp.py',
                'scripts/storage_manager.py'
            ]
            
            passed_scripts = 0
            total_scripts = len(integration_scripts)
            test_details = []
            
            for script in integration_scripts:
                script_path = self.project_root / script
                if script_path.exists():
                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œå¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                    try:
                        result = subprocess.run([
                            sys.executable, '-m', 'py_compile', str(script_path)
                        ], capture_output=True, text=True, timeout=30)
                        
                        if result.returncode == 0:
                            passed_scripts += 1
                            test_details.append({'script': script, 'status': 'passed'})
                        else:
                            test_details.append({'script': script, 'status': 'failed', 'error': result.stderr})
                    except Exception as e:
                        test_details.append({'script': script, 'status': 'error', 'error': str(e)})
                else:
                    test_details.append({'script': script, 'status': 'missing'})
            
            return {
                'passed': passed_scripts == total_scripts,
                'passed_scripts': passed_scripts,
                'total_scripts': total_scripts,
                'score': (passed_scripts / total_scripts) * 100,
                'details': test_details
            }
            
        except Exception as e:
            logger.error(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _run_syntax_checks(self) -> Dict[str, Any]:
        """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        try:
            python_files = list(self.project_root.rglob("*.py"))
            passed_files = 0
            syntax_errors = []
            
            for py_file in python_files:
                try:
                    with open(py_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    compile(content, py_file, 'exec')
                    passed_files += 1
                except SyntaxError as e:
                    syntax_errors.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'error': str(e),
                        'line': e.lineno
                    })
                except Exception as e:
                    syntax_errors.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'error': str(e)
                    })
            
            return {
                'passed': len(syntax_errors) == 0,
                'total_files': len(python_files),
                'passed_files': passed_files,
                'syntax_errors': syntax_errors,
                'score': (passed_files / len(python_files)) * 100 if python_files else 100
            }
            
        except Exception as e:
            logger.error(f"æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {'passed': False, 'error': str(e)}
    
    def _run_import_checks(self) -> Dict[str, Any]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        try:
            core_modules = [
                'scripts.utils',
                'scripts.generate_article',
                'scripts.fetch_image',
                'scripts.post_to_wp',
                'scripts.storage_manager'
            ]
            
            import_results = []
            successful_imports = 0
            
            for module_name in core_modules:
                try:
                    # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å­˜åœ¨ç¢ºèª
                    module_path = self.project_root / module_name.replace('.', '/')
                    if module_path.with_suffix('.py').exists():
                        # ç›¸å¯¾ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã—ã¦ç¢ºèª
                        importlib.import_module(module_name)
                        import_results.append({'module': module_name, 'status': 'success'})
                        successful_imports += 1
                    else:
                        import_results.append({'module': module_name, 'status': 'missing'})
                except ImportError as e:
                    import_results.append({'module': module_name, 'status': 'import_error', 'error': str(e)})
                except Exception as e:
                    import_results.append({'module': module_name, 'status': 'error', 'error': str(e)})
            
            return {
                'passed': successful_imports == len(core_modules),
                'successful_imports': successful_imports,
                'total_modules': len(core_modules),
                'score': (successful_imports / len(core_modules)) * 100,
                'details': import_results
            }
            
        except Exception as e:
            logger.error(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {'passed': False, 'error': str(e)}
    
    def check_code_quality(self) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ“Š ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        quality_results = {
            'file_structure': self._check_file_structure(),
            'documentation': self._check_documentation_quality(),
            'configuration': self._check_configuration_files(),
            'dependencies': self._check_dependencies()
        }
        
        # å“è³ªç·åˆè©•ä¾¡
        passed_checks = sum(1 for result in quality_results.values() if result.get('passed', False))
        total_checks = len(quality_results)
        quality_results['overall_score'] = (passed_checks / total_checks) * 100
        
        logger.info(f"âœ… ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†: {passed_checks}/{total_checks} åˆæ ¼")
        return quality_results
    
    def _check_file_structure(self) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ãƒã‚§ãƒƒã‚¯"""
        required_files = [
            'README.md',
            'requirements.txt',
            '.env.example',
            'scripts/utils.py',
            'scripts/generate_article.py',
            'scripts/fetch_image.py',
            'scripts/post_to_wp.py',
            '.github/workflows/daily-blog.yml'
        ]
        
        required_dirs = [
            'scripts/',
            'prompts/',
            'output/',
            'tests/',
            '.github/workflows/'
        ]
        
        missing_files = []
        missing_dirs = []
        
        for file_path in required_files:
            if not (self.project_root / file_path).exists():
                missing_files.append(file_path)
        
        for dir_path in required_dirs:
            if not (self.project_root / dir_path).exists():
                missing_dirs.append(dir_path)
        
        return {
            'passed': len(missing_files) == 0 and len(missing_dirs) == 0,
            'required_files': len(required_files),
            'existing_files': len(required_files) - len(missing_files),
            'required_dirs': len(required_dirs),
            'existing_dirs': len(required_dirs) - len(missing_dirs),
            'missing_files': missing_files,
            'missing_dirs': missing_dirs,
            'score': ((len(required_files) - len(missing_files) + len(required_dirs) - len(missing_dirs)) / 
                     (len(required_files) + len(required_dirs))) * 100
        }
    
    def _check_documentation_quality(self) -> Dict[str, Any]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ªãƒã‚§ãƒƒã‚¯"""
        readme_path = self.project_root / "README.md"
        
        if not readme_path.exists():
            return {'passed': False, 'error': 'README.md not found'}
        
        content = readme_path.read_text(encoding='utf-8')
        
        required_sections = [
            '# ',  # ã‚¿ã‚¤ãƒˆãƒ«
            '## ',  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦‹å‡ºã—
            '```',  # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
            'http'  # URL
        ]
        
        found_sections = sum(1 for section in required_sections if section in content)
        
        return {
            'passed': found_sections >= len(required_sections) * 0.8,  # 80%ä»¥ä¸Š
            'content_length': len(content),
            'found_sections': found_sections,
            'required_sections': len(required_sections),
            'score': (found_sections / len(required_sections)) * 100,
            'has_code_examples': '```' in content,
            'has_links': 'http' in content
        }
    
    def _check_configuration_files(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯"""
        config_files = {
            '.env.example': ['ANTHROPIC_API_KEY', 'WP_USER', 'WP_APP_PASS'],
            'requirements.txt': ['anthropic', 'requests', 'markdown'],
            '.github/workflows/daily-blog.yml': ['schedule:', 'cron:', 'python']
        }
        
        results = {}
        all_passed = True
        
        for config_file, required_content in config_files.items():
            file_path = self.project_root / config_file
            
            if file_path.exists():
                content = file_path.read_text(encoding='utf-8')
                found_content = [item for item in required_content if item in content]
                
                file_result = {
                    'exists': True,
                    'required_content': len(required_content),
                    'found_content': len(found_content),
                    'score': (len(found_content) / len(required_content)) * 100,
                    'passed': len(found_content) >= len(required_content) * 0.8
                }
                
                if not file_result['passed']:
                    all_passed = False
                    
                results[config_file] = file_result
            else:
                results[config_file] = {'exists': False, 'passed': False}
                all_passed = False
        
        return {
            'passed': all_passed,
            'files': results,
            'total_files': len(config_files),
            'existing_files': sum(1 for r in results.values() if r.get('exists', False))
        }
    
    def _check_dependencies(self) -> Dict[str, Any]:
        """ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯"""
        req_file = self.project_root / "requirements.txt"
        
        if not req_file.exists():
            return {'passed': False, 'error': 'requirements.txt not found'}
        
        content = req_file.read_text(encoding='utf-8')
        lines = [line.strip() for line in content.split('\n') if line.strip() and not line.startswith('#')]
        
        core_dependencies = [
            'anthropic',
            'requests',
            'markdown',
            'jinja2',
            'Pillow'
        ]
        
        found_deps = sum(1 for dep in core_dependencies if any(dep in line for line in lines))
        
        return {
            'passed': found_deps >= len(core_dependencies),
            'total_dependencies': len(lines),
            'core_dependencies': len(core_dependencies),
            'found_core_deps': found_deps,
            'score': (found_deps / len(core_dependencies)) * 100,
            'dependency_list': lines[:10]  # æœ€åˆã®10å€‹
        }
    
    def check_security(self) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        security_results = {
            'secret_exposure': self._check_secret_exposure(),
            'input_validation': self._check_input_validation(),
            'https_usage': self._check_https_usage(),
            'file_permissions': self._check_file_permissions()
        }
        
        passed_checks = sum(1 for result in security_results.values() if result.get('passed', False))
        total_checks = len(security_results)
        security_results['overall_score'] = (passed_checks / total_checks) * 100
        
        logger.info(f"âœ… ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯å®Œäº†: {passed_checks}/{total_checks} åˆæ ¼")
        return security_results
    
    def _check_secret_exposure(self) -> Dict[str, Any]:
        """ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆéœ²å‡ºãƒã‚§ãƒƒã‚¯"""
        dangerous_patterns = [
            'api_key = "',
            'password = "',
            'secret = "',
            'token = "',
            'ANTHROPIC_API_KEY=sk-',
            'OPENAI_API_KEY=sk-'
        ]
        
        exposed_secrets = []
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                for pattern in dangerous_patterns:
                    if pattern in content:
                        exposed_secrets.append({
                            'file': str(py_file.relative_to(self.project_root)),
                            'pattern': pattern
                        })
            except Exception:
                continue
        
        return {
            'passed': len(exposed_secrets) == 0,
            'exposed_secrets': exposed_secrets,
            'checked_patterns': len(dangerous_patterns),
            'score': 100 if len(exposed_secrets) == 0 else 0
        }
    
    def _check_input_validation(self) -> Dict[str, Any]:
        """å…¥åŠ›æ¤œè¨¼ãƒã‚§ãƒƒã‚¯"""
        validation_patterns = [
            'clean_html_content',
            'validate_',
            'if not ',
            'raise ValueError',
            'except '
        ]
        
        validation_found = []
        total_py_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            total_py_files += 1
            try:
                content = py_file.read_text(encoding='utf-8')
                file_validations = []
                
                for pattern in validation_patterns:
                    if pattern in content:
                        file_validations.append(pattern)
                
                if file_validations:
                    validation_found.append({
                        'file': str(py_file.relative_to(self.project_root)),
                        'validations': file_validations
                    })
            except Exception:
                continue
        
        return {
            'passed': len(validation_found) >= total_py_files * 0.5,  # 50%ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã§æ¤œè¨¼
            'files_with_validation': len(validation_found),
            'total_python_files': total_py_files,
            'score': (len(validation_found) / total_py_files) * 100 if total_py_files > 0 else 100,
            'validation_details': validation_found[:5]  # æœ€åˆã®5å€‹
        }
    
    def _check_https_usage(self) -> Dict[str, Any]:
        """HTTPSä½¿ç”¨ãƒã‚§ãƒƒã‚¯"""
        https_count = 0
        http_count = 0
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                https_count += content.count('https://')
                http_count += content.count('http://')
            except Exception:
                continue
        
        total_urls = https_count + http_count
        
        return {
            'passed': http_count == 0 or (total_urls > 0 and https_count / total_urls >= 0.8),
            'https_urls': https_count,
            'http_urls': http_count,
            'total_urls': total_urls,
            'https_ratio': (https_count / total_urls) * 100 if total_urls > 0 else 100,
            'score': (https_count / total_urls) * 100 if total_urls > 0 else 100
        }
    
    def _check_file_permissions(self) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ãƒã‚§ãƒƒã‚¯"""
        sensitive_files = [
            '.env.example',
            'scripts/*.py'
        ]
        
        permission_issues = []
        checked_files = 0
        
        for pattern in sensitive_files:
            for file_path in self.project_root.glob(pattern):
                checked_files += 1
                try:
                    # åŸºæœ¬çš„ãªèª­ã¿å–ã‚Šå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
                    if file_path.is_file() and file_path.exists():
                        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€èª­ã¿å–ã‚Šå¯èƒ½
                        continue
                    else:
                        permission_issues.append(str(file_path.relative_to(self.project_root)))
                except Exception as e:
                    permission_issues.append(f"{file_path.relative_to(self.project_root)}: {e}")
        
        return {
            'passed': len(permission_issues) == 0,
            'checked_files': checked_files,
            'permission_issues': permission_issues,
            'score': ((checked_files - len(permission_issues)) / checked_files) * 100 if checked_files > 0 else 100
        }
    
    def check_performance(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯"""
        logger.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        performance_results = {
            'code_efficiency': self._check_code_efficiency(),
            'resource_usage': self._check_resource_usage(),
            'api_optimization': self._check_api_optimization()
        }
        
        passed_checks = sum(1 for result in performance_results.values() if result.get('passed', False))
        total_checks = len(performance_results)
        performance_results['overall_score'] = (passed_checks / total_checks) * 100
        
        logger.info(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯å®Œäº†: {passed_checks}/{total_checks} åˆæ ¼")
        return performance_results
    
    def _check_code_efficiency(self) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰åŠ¹ç‡æ€§ãƒã‚§ãƒƒã‚¯"""
        efficiency_patterns = [
            'logger',
            'try:',
            'except',
            'return',
            'def '
        ]
        
        efficient_files = 0
        total_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            total_files += 1
            try:
                content = py_file.read_text(encoding='utf-8')
                found_patterns = sum(1 for pattern in efficiency_patterns if pattern in content)
                
                if found_patterns >= len(efficiency_patterns) * 0.4:  # 40%ä»¥ä¸Šã«ç·©å’Œ
                    efficient_files += 1
            except Exception:
                continue
        
        return {
            'passed': efficient_files >= total_files * 0.5,  # 50%ä»¥ä¸Šã«ç·©å’Œ
            'efficient_files': efficient_files,
            'total_files': total_files,
            'score': (efficient_files / total_files) * 100 if total_files > 0 else 100
        }
    
    def _check_resource_usage(self) -> Dict[str, Any]:
        """ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯"""
        # åŸºæœ¬çš„ãªãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        resource_patterns = [
            'close()',
            'with open',
            'context manager',
            'finally:',
            'cleanup'
        ]
        
        good_practices = 0
        total_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            total_files += 1
            try:
                content = py_file.read_text(encoding='utf-8')
                if any(pattern in content for pattern in resource_patterns):
                    good_practices += 1
            except Exception:
                continue
        
        return {
            'passed': good_practices >= total_files * 0.3,  # 30%ä»¥ä¸Šã«ç·©å’Œ
            'files_with_good_practices': good_practices,
            'total_files': total_files,
            'score': (good_practices / total_files) * 100 if total_files > 0 else 100
        }
    
    def _check_api_optimization(self) -> Dict[str, Any]:
        """APIæœ€é©åŒ–ãƒã‚§ãƒƒã‚¯"""
        optimization_patterns = [
            'retry',
            'timeout',
            'rate_limit',
            'cache',
            'mock'
        ]
        
        optimized_files = 0
        api_files = 0
        
        for py_file in self.project_root.rglob("*.py"):
            try:
                content = py_file.read_text(encoding='utf-8')
                
                # APIã‚’ä½¿ç”¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
                if any(api_word in content.lower() for api_word in ['api', 'http', 'request']):
                    api_files += 1
                    
                    # æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
                    if any(pattern in content.lower() for pattern in optimization_patterns):
                        optimized_files += 1
            except Exception:
                continue
        
        return {
            'passed': optimized_files >= api_files * 0.5 if api_files > 0 else True,  # 50%ä»¥ä¸Šã«ç·©å’Œ
            'optimized_api_files': optimized_files,
            'total_api_files': api_files,
            'score': (optimized_files / api_files) * 100 if api_files > 0 else 100
        }
    
    def check_completion_status(self) -> Dict[str, Any]:
        """å®Œæˆåº¦ãƒã‚§ãƒƒã‚¯"""
        logger.info("ğŸ¯ å®Œæˆåº¦ãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        completion_items = {
            'core_scripts': self._check_core_scripts_completion(),
            'documentation': self._check_documentation_completion(),
            'configuration': self._check_configuration_completion(),
            'testing': self._check_testing_completion(),
            'deployment': self._check_deployment_readiness()
        }
        
        # å®Œæˆåº¦ç·åˆè©•ä¾¡
        total_score = sum(item.get('score', 0) for item in completion_items.values())
        completion_items['overall_completion'] = total_score / len(completion_items)
        
        logger.info(f"âœ… å®Œæˆåº¦ãƒã‚§ãƒƒã‚¯å®Œäº†: {completion_items['overall_completion']:.1f}%")
        return completion_items
    
    def _check_core_scripts_completion(self) -> Dict[str, Any]:
        """ã‚³ã‚¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œæˆåº¦ãƒã‚§ãƒƒã‚¯"""
        core_scripts = [
            'scripts/generate_article.py',
            'scripts/fetch_image.py',
            'scripts/post_to_wp.py',
            'scripts/utils.py',
            'scripts/storage_manager.py'
        ]
        
        completed_scripts = 0
        script_details = []
        
        for script in core_scripts:
            script_path = self.project_root / script
            if script_path.exists():
                try:
                    content = script_path.read_text(encoding='utf-8')
                    
                    # åŸºæœ¬çš„ãªå®Œæˆåº¦æŒ‡æ¨™
                    has_main = 'if __name__' in content
                    has_logging = 'logger' in content
                    has_error_handling = 'try:' in content and 'except' in content
                    has_docstring = '"""' in content
                    
                    completion_score = sum([has_main, has_logging, has_error_handling, has_docstring]) * 25
                    
                    if completion_score >= 75:  # 75%ä»¥ä¸Šã§å®Œæˆã¨åˆ¤å®š
                        completed_scripts += 1
                    
                    script_details.append({
                        'script': script,
                        'completion_score': completion_score,
                        'has_main': has_main,
                        'has_logging': has_logging,
                        'has_error_handling': has_error_handling,
                        'has_docstring': has_docstring
                    })
                except Exception as e:
                    script_details.append({
                        'script': script,
                        'completion_score': 0,
                        'error': str(e)
                    })
            else:
                script_details.append({
                    'script': script,
                    'completion_score': 0,
                    'missing': True
                })
        
        return {
            'completed_scripts': completed_scripts,
            'total_scripts': len(core_scripts),
            'score': (completed_scripts / len(core_scripts)) * 100,
            'details': script_details
        }
    
    def _check_documentation_completion(self) -> Dict[str, Any]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œæˆåº¦ãƒã‚§ãƒƒã‚¯"""
        required_docs = {
            'README.md': ['æ¦‚è¦', 'ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«', 'ä½¿ç”¨æ–¹æ³•', 'API'],
            '.env.example': ['ANTHROPIC_API_KEY', 'WP_USER'],
            'specifications/project_spec.md': ['ç›®çš„', 'ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼']
        }
        
        completed_docs = 0
        doc_details = []
        
        for doc_file, required_content in required_docs.items():
            doc_path = self.project_root / doc_file
            
            if doc_path.exists():
                try:
                    content = doc_path.read_text(encoding='utf-8')
                    found_content = sum(1 for item in required_content if item in content)
                    completion_score = (found_content / len(required_content)) * 100
                    
                    if completion_score >= 70:  # 70%ä»¥ä¸Šã§å®Œæˆã¨åˆ¤å®š
                        completed_docs += 1
                    
                    doc_details.append({
                        'document': doc_file,
                        'completion_score': completion_score,
                        'found_content': found_content,
                        'required_content': len(required_content)
                    })
                except Exception as e:
                    doc_details.append({
                        'document': doc_file,
                        'completion_score': 0,
                        'error': str(e)
                    })
            else:
                doc_details.append({
                    'document': doc_file,
                    'completion_score': 0,
                    'missing': True
                })
        
        return {
            'completed_docs': completed_docs,
            'total_docs': len(required_docs),
            'score': (completed_docs / len(required_docs)) * 100,
            'details': doc_details
        }
    
    def _check_configuration_completion(self) -> Dict[str, Any]:
        """è¨­å®šå®Œæˆåº¦ãƒã‚§ãƒƒã‚¯"""
        config_items = [
            ('.env.example', 'Environment variables template'),
            ('requirements.txt', 'Python dependencies'),
            ('.github/workflows/daily-blog.yml', 'GitHub Actions workflow')
        ]
        
        completed_configs = 0
        config_details = []
        
        for config_file, description in config_items:
            config_path = self.project_root / config_file
            
            if config_path.exists() and config_path.stat().st_size > 0:
                completed_configs += 1
                config_details.append({
                    'config': config_file,
                    'description': description,
                    'status': 'completed',
                    'size': config_path.stat().st_size
                })
            else:
                config_details.append({
                    'config': config_file,
                    'description': description,
                    'status': 'missing' if not config_path.exists() else 'empty'
                })
        
        return {
            'completed_configs': completed_configs,
            'total_configs': len(config_items),
            'score': (completed_configs / len(config_items)) * 100,
            'details': config_details
        }
    
    def _check_testing_completion(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆå®Œæˆåº¦ãƒã‚§ãƒƒã‚¯"""
        test_components = [
            ('tests/test_basic.py', 'Basic functionality tests'),
            ('scripts/utils.py', 'Utility functions'),
            ('output/', 'Output directory')
        ]
        
        completed_tests = 0
        test_details = []
        
        for test_item, description in test_components:
            test_path = self.project_root / test_item
            
            if test_path.exists():
                completed_tests += 1
                test_details.append({
                    'test': test_item,
                    'description': description,
                    'status': 'completed'
                })
            else:
                test_details.append({
                    'test': test_item,
                    'description': description,
                    'status': 'missing'
                })
        
        return {
            'completed_tests': completed_tests,
            'total_tests': len(test_components),
            'score': (completed_tests / len(test_components)) * 100,
            'details': test_details
        }
    
    def _check_deployment_readiness(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™å®Œæˆåº¦ãƒã‚§ãƒƒã‚¯"""
        deployment_items = [
            ('.github/workflows/daily-blog.yml', 'GitHub Actions workflow'),
            ('requirements.txt', 'Dependencies defined'),
            ('.env.example', 'Environment template'),
            ('README.md', 'Documentation')
        ]
        
        ready_items = 0
        deployment_details = []
        
        for item_file, description in deployment_items:
            item_path = self.project_root / item_file
            
            if item_path.exists() and item_path.stat().st_size > 100:  # æœ€ä½é™ã®ã‚µã‚¤ã‚º
                ready_items += 1
                deployment_details.append({
                    'item': item_file,
                    'description': description,
                    'status': 'ready'
                })
            else:
                deployment_details.append({
                    'item': item_file,
                    'description': description,
                    'status': 'not_ready'
                })
        
        return {
            'ready_items': ready_items,
            'total_items': len(deployment_items),
            'score': (ready_items / len(deployment_items)) * 100,
            'details': deployment_details
        }
    
    def calculate_overall_score(self) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        weight_map = {
            'test_results': 0.3,
            'code_quality': 0.25,
            'security': 0.2,
            'performance': 0.15,
            'completion_status': 0.1
        }
        
        total_score = 0
        for category, weight in weight_map.items():
            if category in self.results:
                category_score = self.results[category].get('overall_score', 0)
                total_score += category_score * weight
        
        return total_score
    
    def run_full_quality_check(self) -> Dict[str, Any]:
        """å®Œå…¨å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        logger.info("ğŸš€ å®Œå…¨å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        self.results['test_results'] = self.run_python_tests()
        self.results['code_quality'] = self.check_code_quality()
        self.results['security'] = self.check_security()
        self.results['performance'] = self.check_performance()
        self.results['completion_status'] = self.check_completion_status()
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        self.results['overall_score'] = self.calculate_overall_score()
        
        # åˆæ ¼åˆ¤å®š
        self.results['passed'] = self.results['overall_score'] >= 75  # 75%ä»¥ä¸Šã§åˆæ ¼
        
        # çµæœä¿å­˜
        save_json_safely(self.results, 'output/quality_check_report.json')
        
        logger.info(f"âœ… å®Œå…¨å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†")
        logger.info(f"ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {self.results['overall_score']:.1f}%")
        logger.info(f"ğŸ¯ åˆæ ¼åˆ¤å®š: {'âœ… åˆæ ¼' if self.results['passed'] else 'âŒ ä¸åˆæ ¼'}")
        
        return self.results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        logger.info("ğŸ” BlogAuto æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹")
        
        # å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
        quality_checker = QualityChecker()
        
        # å®Œå…¨å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        results = quality_checker.run_full_quality_check()
        
        # çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›
        print("\n" + "="*60)
        print("ğŸ¯ BlogAuto æœ€çµ‚å“è³ªãƒã‚§ãƒƒã‚¯çµæœ")
        print("="*60)
        print(f"ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {results['overall_score']:.1f}%")
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœ: {results['test_results']['overall_score']:.1f}%")
        print(f"ğŸ“‹ ã‚³ãƒ¼ãƒ‰å“è³ª: {results['code_quality']['overall_score']:.1f}%")
        print(f"ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: {results['security']['overall_score']:.1f}%")
        print(f"âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {results['performance']['overall_score']:.1f}%")
        print(f"ğŸ¯ å®Œæˆåº¦: {results['completion_status']['overall_completion']:.1f}%")
        print(f"ğŸ† æœ€çµ‚åˆ¤å®š: {'âœ… åˆæ ¼' if results['passed'] else 'âŒ ä¸åˆæ ¼'}")
        print("="*60)
        
        return results['passed']
        
    except Exception as e:
        logger.error(f"âŒ å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)